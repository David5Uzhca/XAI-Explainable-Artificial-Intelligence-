# XAI ‚Äì Interpretaci√≥n de Modelos de Machine Learning y Deep Learning

## Descripci√≥n general del proyecto

Este proyecto presenta un **estudio t√©cnico y experimental sobre Explainable Artificial Intelligence (XAI)**, aplicando y comparando modelos de *Machine Learning cl√°sico* y *Deep Learning* sobre **datos estructurados** e **im√°genes**, con el objetivo de **interpretar, analizar y explicar las predicciones** de cada modelo.

El trabajo se centra en responder una pregunta clave:

> ¬øC√≥mo y por qu√© un modelo toma una decisi√≥n, m√°s all√° de su precisi√≥n?

Para ello, se desarrollaron **dos pr√°cticas complementarias**, integradas bajo un mismo marco conceptual de explicabilidad.

---

## Objetivos

### Objetivo general

Analizar y comparar modelos predictivos desde la perspectiva de **XAI**, evaluando su desempe√±o y su capacidad de explicaci√≥n.

### Objetivos espec√≠ficos

* Aplicar modelos interpretables y no interpretables en distintos tipos de datos.
* Identificar **caracter√≠sticas importantes** y **factores clave** en las predicciones.
* Visualizar y analizar explicaciones globales y locales.
* Evidenciar las limitaciones de la interpretabilidad en modelos complejos.

---

## Marco conceptual: XAI

**Explainable Artificial Intelligence (XAI)** busca hacer transparentes los modelos de IA, permitiendo:

* Comprender decisiones del modelo
* Generar confianza
* Detectar sesgos
* Facilitar la toma de decisiones humanas

Este proyecto contrasta:

* **Modelos inherentemente interpretables** (√Årboles, Random Forest, SVM)
* **Modelos de alta complejidad** (Redes Neuronales, CNN)

---

## Pr√°ctica 1: Datos estructurados

### Dataset

**Online Shoppers Purchasing Intention Dataset**
[https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset](https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset)

* Variables num√©ricas y categ√≥ricas
* Variable objetivo: `Revenue` (0 = No compra, 1 = Compra)
* Problema: **Clasificaci√≥n binaria**

### Modelos aplicados

* **Random Forest / √Årbol de Decisi√≥n**
* **Red Neuronal Artificial (MLP)**

### Flujo del proceso

1. Carga de datos y an√°lisis exploratorio (EDA)
2. Preprocesamiento:

   * Variables dummy
   * Escalado con `StandardScaler`
3. Entrenamiento de modelos
4. Evaluaci√≥n de m√©tricas
5. An√°lisis XAI

### Caracter√≠sticas importantes (XAI)

* `PageValue` ‚Üí Variable m√°s influyente
* `ExitRates` y `BounceRates`
* `ProductRelated` y duraci√≥n
* `VisitorType`

### Resultados clave

* Ambos modelos identifican **PageValue** como factor determinante
* El **√Årbol de Decisi√≥n** reduce falsos negativos
* Mejor interpretaci√≥n de patrones l√≥gicos de compra

### Ventajas y desventajas

**Random Forest / √Årbol**

* ‚úî Alta interpretabilidad
* ‚úî Explicaci√≥n directa de decisiones
* ‚úñ Menor capacidad para relaciones muy complejas

**Red Neuronal**

* ‚úî Mayor capacidad de representaci√≥n
* ‚úñ Modelo tipo *caja negra*

### Conclusi√≥n pr√°ctica 1

Los modelos basados en √°rboles resultan m√°s adecuados cuando la **explicabilidad es prioritaria**, especialmente en contextos de negocio.

---

## Pr√°ctica 2: Clasificaci√≥n de im√°genes (Perros vs Gatos)

### Dataset

**Oxford-IIIT Pet Dataset**  
[https://www.kaggle.com/datasets/tanlikesmath/the-oxfordiiit-pet-dataset](https://www.kaggle.com/datasets/tanlikesmath/the-oxfordiiit-pet-dataset)

- Im√°genes RGB de 37 razas de gatos y perros (en esta pr√°ctica: subconjunto binario **Gato vs Perro**)
- Alta variabilidad visual (posturas, iluminaci√≥n, fondo, raza)
- **Tarea**: Clasificaci√≥n binaria de im√°genes

### Modelos aplicados

- **SVM** (LinearSVC) ‚Äì baseline cl√°sico
- **CNN** (Convolutional Neural Network) ‚Äì enfoque deep learning

### Flujo del proceso

1. Carga y preprocesamiento de im√°genes
2. Conversi√≥n a escala de grises + redimensionamiento (64√ó64)
3. Vectorizaci√≥n de im√°genes (para SVM)
4. Divisi√≥n train / validation / test
5. Entrenamiento de ambos modelos
6. Evaluaci√≥n + visualizaci√≥n de explicabilidad (XAI)

### Enfoque XAI

- **SVM**: Visualizaci√≥n de los pesos del clasificador como mapa de calor
- **CNN**: An√°lisis conceptual de regiones activadas (enfoque cualitativo en esta pr√°ctica)

### Resultados clave

#### SVM
- Genera mapas de pesos **ruidosos** y poco interpretables
- Busca patrones espaciales r√≠gidos ‚Üí no captura bien la variabilidad natural de las im√°genes
- Rendimiento inferior al de la CNN (no se detalla aqu√≠ la m√©trica exacta, pero visiblemente peor)

#### CNN
- **Accuracy final en conjunto de test**: **80.58%**
- Comportamiento durante el entrenamiento:
  - Accuracy en entrenamiento ‚Üí sube de forma sostenida hasta ~98%
  - Accuracy en validaci√≥n ‚Üí se estabiliza alrededor de **~80‚Äì82%** (evidencia clara de **sobreajuste**)
  - P√©rdida (loss) en entrenamiento ‚Üí disminuye correctamente
  - P√©rdida en validaci√≥n ‚Üí **aumenta** significativamente despu√©s de varias √©pocas ‚Üí confirma sobreajuste

**Matriz de confusi√≥n (test)**

| True \ Predicted | Gato       | Perro      |
|------------------|------------|------------|
| **Gato**         | 288        | 192        |
| **Perro**        | 95         | 903        |

- Total de ejemplos de gatos: 288 + 192 = **480**
- Total de ejemplos de perros: 95 + 903 = **998**
- **Fuerte sesgo hacia la clase "Perro"** (el modelo predice "Perro" en la mayor√≠a de los casos)
- Recall para "Gato" ‚âà 60% (288/480) ‚Üí muchos gatos clasificados err√≥neamente como perros
- Recall para "Perro" ‚âà 90.5% (903/998) ‚Üí muy buen desempe√±o en la clase mayoritaria

### Limitaciones detectadas

- **Sobreajuste evidente** en la CNN (divergencia train/val loss)
- Desbalance de clases en el conjunto de evaluaci√≥n (~2:1 perros:gatos) ‚Üí sesgo hacia la clase mayoritaria
- Preprocesamiento simple (grises + 64√ó64) ‚Üí p√©rdida importante de informaci√≥n (color y resoluci√≥n)
- El SVM, aunque matem√°ticamente interpretable, **no captura jerarqu√≠as visuales** ‚Üí mapas de pesos poco sem√°nticos
- Explicabilidad matem√°tica (SVM) ‚â† explicabilidad sem√°ntica/humanamente comprensible

### Conclusi√≥n pr√°ctica 2

Aunque el **SVM** ofrece explicabilidad directa a nivel de p√≠xeles, **no es adecuado** para tareas de visi√≥n por computador con alta variabilidad como esta.  
Las **CNN** logran un rendimiento significativamente superior (**80.58%** en test), pero muestran **sobreajuste** 

---

## Relaci√≥n global con XAI

Este proyecto demuestra que:

* La precisi√≥n no es suficiente
* La interpretabilidad depende del tipo de dato
* XAI act√∫a como puente entre modelos y usuarios

| Tipo de modelo | Precisi√≥n | Explicabilidad |
| -------------- | --------- | -------------- |
| √Årboles        | Media     | Alta           |
| Random Forest  | Alta      | Media-Alta     |
| SVM            | Media     | Media          |
| CNN            | Muy alta  | Baja (sin XAI) |

---

## üìÅ Estructura del proyecto

```
Proyecto-XAI
 ‚î£ üìÇ data
 ‚î£ üìÇ notebooks
 ‚îÉ ‚î£ DecisionTree.ipynb
 ‚îÉ ‚î£ SMV.ipynb
 ‚îÉ ‚î£ CNN.ipynb
 ‚î£ üìÑ README.md
 ‚îó üìÑ X-ai.pdf
```

---

## Autores

* **David Uzhca**
* **Domenika Delgado**
* **Irar Nankamai**

---

## Conclusi√≥n general

La **Explainable AI** es esencial para una adopci√≥n responsable de modelos de IA. Este proyecto evidencia que **comprender el porqu√© de una predicci√≥n es tan importante como el resultado mismo**, especialmente en contextos reales donde la confianza y la transparencia son fundamentales.
Depende mucho del conte
